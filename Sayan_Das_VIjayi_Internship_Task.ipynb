{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PART A: Task: Extract and Categorize Tasks from Unannotated Text (70 marks)"
      ],
      "metadata": {
        "id": "4-CuNpWsRytI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsdZEIbCzWVw"
      },
      "source": [
        "## NLP Pipeline for Task Extraction and Categorization\n",
        "\n",
        "This notebook demonstrates an NLP pipeline that extracts tasks from unstructured text and categorizes them using clustering and topic modeling. The pipeline includes:\n",
        "\n",
        "1. **Preprocessing:** Clean and segment input text into sentences.\n",
        "2. **Task Extraction:** Identify task sentences using heuristics and extract details such as performer and deadline.\n",
        "3. **Clustering:** Compute sentence embeddings, then determine the optimal number of clusters using either the _Elbow Method_ or the _Silhouette Score_.\n",
        "4. **Categorization:** Use LDA topic modeling on each cluster to derive a category label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6bt0pKKwzIwg",
        "outputId": "9c09b39b-07af-4d68-92ad-e2b4ea5670b7"
      },
      "source": [
        "# Install required packages\n",
        "!pip install spacy scikit-learn gensim matplotlib\n",
        "!python -m spacy download en_core_web_md"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-md==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-md==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries and load the spaCy model\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pprint import pprint\n",
        "\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from gensim import corpora, models\n",
        "\n",
        "# Load spaCy model with medium-sized vectors\n",
        "nlp = spacy.load('en_core_web_md')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lrRhFcXVNS3",
        "outputId": "3c56d27f-b4b2-4f39-f3f7-d172c2acade9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utility Functions"
      ],
      "metadata": {
        "id": "WUTETijkVTIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_text_from_file(filepath):\n",
        "    \"\"\"\n",
        "    Read text from the provided file path.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
        "            text = file.read()\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        raise IOError(f\"Error reading file '{filepath}': {e}\")\n",
        "\n",
        "def pretty_print_tasks(tasks, title=\"Tasks:\"):\n",
        "    \"\"\"\n",
        "    Pretty-print the list of task dictionaries in JSON format.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{title}\")\n",
        "    print(json.dumps(tasks, indent=4))"
      ],
      "metadata": {
        "id": "x8N6tF76Vjmi"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Functions"
      ],
      "metadata": {
        "id": "2AOdrusQVmY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean the input text by stripping whitespace.\n",
        "    \"\"\"\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "p9x39CHBVpnh"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task Identification and Extraction"
      ],
      "metadata": {
        "id": "qQaxg3j9VtwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_task_sentence(sentence):\n",
        "    \"\"\"\n",
        "    Determine if a sentence likely represents a task.\n",
        "    Uses two heuristics:\n",
        "      1. Check if the sentence starts with a base form verb.\n",
        "      2. Check for common task-related keywords (e.g., \"has to\", \"please\", \"don't forget\").\n",
        "    \"\"\"\n",
        "    sent_text = sentence.text.strip()\n",
        "    sent_lower = sent_text.lower()\n",
        "\n",
        "    # Keywords that may indicate a task\n",
        "    task_keywords = ['has to', 'need to', 'needs to', 'should', 'must', 'please', \"don't forget\"]\n",
        "\n",
        "    # Heuristic 1: Check if the first token is a base form verb\n",
        "    first_token = sentence[0]\n",
        "    if first_token.pos_ == 'VERB' and first_token.tag_ == 'VB':\n",
        "        return True\n",
        "\n",
        "    # Heuristic 2: Look for task-related keywords\n",
        "    for keyword in task_keywords:\n",
        "        if keyword in sent_lower:\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def extract_deadline(sentence):\n",
        "    \"\"\"\n",
        "    Extract deadline information from a sentence by combining adjacent\n",
        "    entities labeled as TIME or DATE.\n",
        "    \"\"\"\n",
        "    deadline_tokens = []\n",
        "    for ent in sentence.ents:\n",
        "        if ent.label_ in [\"TIME\", \"DATE\"]:\n",
        "            deadline_tokens.append(ent.text)\n",
        "    if deadline_tokens:\n",
        "        return \" \".join(deadline_tokens)\n",
        "    return None\n",
        "\n",
        "def extract_task_details(sentence):\n",
        "    \"\"\"\n",
        "    Extract details from a task sentence:\n",
        "      - The full task text\n",
        "      - The performer (first PERSON entity, if any)\n",
        "      - The deadline (if any)\n",
        "    \"\"\"\n",
        "    task_text = sentence.text.strip()\n",
        "    performer = None\n",
        "\n",
        "    # Find the first PERSON entity\n",
        "    for ent in sentence.ents:\n",
        "        if ent.label_ == \"PERSON\":\n",
        "            performer = ent.text\n",
        "            break\n",
        "\n",
        "    deadline = extract_deadline(sentence)\n",
        "    return task_text, performer, deadline\n",
        "\n",
        "def process_text(text):\n",
        "    \"\"\"\n",
        "    Process the input text:\n",
        "      - Clean the text\n",
        "      - Segment it into sentences\n",
        "      - Extract sentences that likely represent tasks\n",
        "    Returns a list of dictionaries containing task details.\n",
        "    \"\"\"\n",
        "    cleaned_text = clean_text(text)\n",
        "    doc = nlp(cleaned_text)\n",
        "    tasks = []\n",
        "\n",
        "    for sent in doc.sents:\n",
        "        if is_task_sentence(sent):\n",
        "            task_text, performer, deadline = extract_task_details(sent)\n",
        "            tasks.append({\n",
        "                \"task\": task_text,\n",
        "                \"performer\": performer,\n",
        "                \"deadline\": deadline\n",
        "            })\n",
        "    return tasks\n"
      ],
      "metadata": {
        "id": "tAfbxRk_Vuhf"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clustering and Categorization Functions"
      ],
      "metadata": {
        "id": "e3jTIdKdV3Ke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cluster_tasks(tasks, num_clusters):\n",
        "    \"\"\"\n",
        "    Cluster tasks based on their sentence embeddings using KMeans.\n",
        "    Adds a 'cluster' field to each task.\n",
        "    Returns the updated tasks, the fitted KMeans model, and the task vectors.\n",
        "    \"\"\"\n",
        "    task_vectors = []\n",
        "    for task in tasks:\n",
        "        doc = nlp(task[\"task\"])\n",
        "        task_vectors.append(doc.vector)\n",
        "    X = np.array(task_vectors)\n",
        "\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "    kmeans.fit(X)\n",
        "    labels = kmeans.labels_\n",
        "\n",
        "    for i, task in enumerate(tasks):\n",
        "        task[\"cluster\"] = int(labels[i])\n",
        "    return tasks, kmeans, X\n",
        "\n",
        "def label_clusters_with_lda(tasks, num_topics=1):\n",
        "    \"\"\"\n",
        "    For each cluster, perform LDA topic modeling on the task texts to derive\n",
        "    a category label from the dominant topic. Adds a 'category' field to each task.\n",
        "    \"\"\"\n",
        "    # Group tasks by cluster\n",
        "    cluster_tasks_dict = {}\n",
        "    for task in tasks:\n",
        "        cluster = task[\"cluster\"]\n",
        "        cluster_tasks_dict.setdefault(cluster, []).append(task[\"task\"])\n",
        "\n",
        "    cluster_labels = {}\n",
        "    for cluster, sentences in cluster_tasks_dict.items():\n",
        "        texts = []\n",
        "        for sentence in sentences:\n",
        "            doc = nlp(sentence)\n",
        "            tokens = [\n",
        "                token.lemma_.lower()\n",
        "                for token in doc\n",
        "                if token.is_alpha and token.text.lower() not in STOP_WORDS\n",
        "            ]\n",
        "            texts.append(tokens)\n",
        "\n",
        "        dictionary = corpora.Dictionary(texts)\n",
        "        corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "        if len(dictionary) == 0:\n",
        "            cluster_labels[cluster] = \"General\"\n",
        "            continue\n",
        "\n",
        "        lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10, random_state=42)\n",
        "        topic_terms = lda_model.show_topic(0, topn=3)\n",
        "        label = \" \".join([word for word, prob in topic_terms])\n",
        "        cluster_labels[cluster] = label\n",
        "\n",
        "    for task in tasks:\n",
        "        task[\"category\"] = cluster_labels[task[\"cluster\"]]\n",
        "    return tasks, cluster_labels\n",
        "\n",
        "def determine_optimal_clusters_elbow(task_vectors, min_clusters=2, max_clusters=10):\n",
        "    \"\"\"\n",
        "    Use the Elbow Method to display a plot of inertia for different cluster counts.\n",
        "    Adjusts max_clusters if there are fewer samples.\n",
        "    \"\"\"\n",
        "    n_samples = task_vectors.shape[0]\n",
        "\n",
        "    if n_samples < min_clusters:\n",
        "        print(f\"Warning: Number of samples ({n_samples}) is less than the minimum clusters ({min_clusters}).\")\n",
        "        min_clusters = n_samples\n",
        "    if n_samples < max_clusters:\n",
        "        max_clusters = n_samples\n",
        "\n",
        "    inertias = []\n",
        "    cluster_range = range(min_clusters, max_clusters + 1)\n",
        "    for k in cluster_range:\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "        kmeans.fit(task_vectors)\n",
        "        inertias.append(kmeans.inertia_)\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(list(cluster_range), inertias, marker=\"o\")\n",
        "    plt.title(\"Elbow Method For Optimal Clusters\")\n",
        "    plt.xlabel(\"Number of clusters\")\n",
        "    plt.ylabel(\"Inertia\")\n",
        "    plt.xticks(list(cluster_range))\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Elbow Method Inertia Values:\")\n",
        "    for k, inertia in zip(cluster_range, inertias):\n",
        "        print(f\"Clusters: {k}, Inertia: {inertia}\")\n",
        "\n",
        "def determine_optimal_clusters_silhouette(task_vectors, min_clusters=2, max_clusters=10):\n",
        "    \"\"\"\n",
        "    Compute the average silhouette score for different numbers of clusters\n",
        "    and return the optimal number of clusters (with the highest score).\n",
        "    Adjusts max_clusters to be at most n_samples - 1.\n",
        "    \"\"\"\n",
        "    n_samples = task_vectors.shape[0]\n",
        "\n",
        "    if n_samples < min_clusters:\n",
        "        min_clusters = n_samples\n",
        "    # Ensure max_clusters does not exceed n_samples - 1\n",
        "    max_clusters = min(max_clusters, n_samples - 1)\n",
        "\n",
        "    silhouette_scores = []\n",
        "    cluster_range = range(min_clusters, max_clusters + 1)\n",
        "    for k in cluster_range:\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "        labels = kmeans.fit_predict(task_vectors)\n",
        "        score = silhouette_score(task_vectors, labels)\n",
        "        silhouette_scores.append(score)\n",
        "        print(f\"Clusters: {k}, Silhouette Score: {score:.3f}\")\n",
        "\n",
        "    optimal_k = cluster_range[np.argmax(silhouette_scores)]\n",
        "    print(f\"\\nOptimal number of clusters based on silhouette score: {optimal_k}\")\n",
        "    return optimal_k\n"
      ],
      "metadata": {
        "id": "LKdm8hi7V3-p"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4lUY9hczxE_"
      },
      "source": [
        "## Running the Pipeline\n",
        "\n",
        "In the cell below, we define the input text (or you can upload a file) and run the complete pipeline:\n",
        "\n",
        "1. **Task Extraction:** Process the input text to extract tasks.\n",
        "2. **Vectorization:** Compute sentence embeddings for each extracted task.\n",
        "3. **Optimal Cluster Determination:** Use the Silhouette Score (or Elbow Method) to choose the number of clusters.\n",
        "4. **Clustering and Categorization:** Cluster the tasks and assign category labels using LDA.\n",
        "5. **Output:** Display the extracted tasks along with their cluster and category information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4Yql55czKGC",
        "outputId": "c84fc9c8-d567-4633-9edb-e29d4d023efb"
      },
      "source": [
        "# Load input text from file \"input.txt\"\n",
        "# (Ensure that input.txt is already uploaded to the Colab working directory. Sample File is provided at the end)\n",
        "input_text = load_text_from_file(\"/content/input.txt\")\n",
        "\n",
        "# Process the input text to extract tasks\n",
        "tasks = process_text(input_text)\n",
        "pretty_print_tasks(tasks, title=\"Extracted Tasks:\")\n",
        "\n",
        "if not tasks:\n",
        "    print(\"No tasks found in the input.\")\n",
        "else:\n",
        "    # Compute task vectors for clustering\n",
        "    task_vectors = []\n",
        "    for task in tasks:\n",
        "        doc = nlp(task[\"task\"])\n",
        "        task_vectors.append(doc.vector)\n",
        "    task_vectors = np.array(task_vectors)\n",
        "\n",
        "    # Choose the method to determine the number of clusters: 'elbow' or 'silhouette'\n",
        "    method = \"elbow\"\n",
        "\n",
        "    if method == \"elbow\":\n",
        "        print(\"\\nDetermining the optimal number of clusters using the Elbow Method...\")\n",
        "        determine_optimal_clusters_elbow(task_vectors, min_clusters=2, max_clusters=10)\n",
        "        # Manually enter the desired number of clusters after reviewing the plot\n",
        "        num_clusters = int(input(\"Based on the elbow plot, enter the desired number of clusters: \"))\n",
        "    else:\n",
        "        print(\"\\nDetermining the optimal number of clusters using the Silhouette Score...\")\n",
        "        num_clusters = determine_optimal_clusters_silhouette(task_vectors, min_clusters=2, max_clusters=10)\n",
        "\n",
        "    # Cluster the tasks and assign category labels using LDA\n",
        "    tasks, kmeans, _ = cluster_tasks(tasks, num_clusters)\n",
        "    tasks, cluster_labels = label_clusters_with_lda(tasks, num_topics=1)\n",
        "\n",
        "    pretty_print_tasks(tasks, title=\"Tasks with Categories:\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracted Tasks:\n",
            "[\n",
            "    {\n",
            "        \"task\": \"Meanwhile, we must finalize the budget by Friday.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": \"Friday\"\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"Rahul and Priya need to schedule the client call by Monday.\",\n",
            "        \"performer\": \"Priya\",\n",
            "        \"deadline\": \"Monday\"\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"Remember to check the new version of the software for bugs.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": null\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"The CEO insisted that marketing should distribute the new brochures next week.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": \"next week\"\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"Bob must complete the security review by 5 PM today.\",\n",
            "        \"performer\": \"Bob\",\n",
            "        \"deadline\": \"5 PM today\"\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"Also, Michael should deliver the updated contract by email.\",\n",
            "        \"performer\": \"Michael\",\n",
            "        \"deadline\": null\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"The CFO said that the financial forecast has to be updated by the end of this quarter.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": \"the end of this quarter\"\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"Could you please forward that email to HR?\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": null\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"Move the old chairs to the storage room.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": null\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"We need to keep track of all the software licenses before they expire.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": null\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"Make sure to call the electrician to fix the broken light.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": null\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"Maria must write the blog post by Tuesday next week.\",\n",
            "        \"performer\": \"Maria\",\n",
            "        \"deadline\": \"Tuesday next week\"\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"Also, the design team should finalize the logo.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": null\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"Prepare a backup copy of all website assets.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": null\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"The cleaning crew must reorder supplies.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": null\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"The CEO has to review the partnership agreement.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": null\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"The marketing team should post the social media updates every morning.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": \"every morning\"\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"We must still complete the daily report.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": \"daily\"\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"We need to deliver these tasks on time.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": null\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"Write the release notes for the software patch.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": null\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"Timmy must update the user guide with the new features by next Wednesday.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": \"next Wednesday\"\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"The finance team has to finalize the Q1 reports by the end of the month.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": \"the end of the month\"\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"We should not forget to track all deliverables in the project management tool.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": null\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"We need to send them our feedback.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": null\n",
            "    }\n",
            "]\n",
            "\n",
            "Determining the optimal number of clusters using the Silhouette Score...\n",
            "Clusters: 2, Silhouette Score: 0.239\n",
            "Clusters: 3, Silhouette Score: 0.163\n",
            "Clusters: 4, Silhouette Score: 0.115\n",
            "Clusters: 5, Silhouette Score: 0.140\n",
            "Clusters: 6, Silhouette Score: 0.067\n",
            "Clusters: 7, Silhouette Score: 0.063\n",
            "Clusters: 8, Silhouette Score: 0.094\n",
            "Clusters: 9, Silhouette Score: 0.095\n",
            "Clusters: 10, Silhouette Score: 0.077\n",
            "\n",
            "Optimal number of clusters based on silhouette score: 2\n",
            "\n",
            "Tasks with Categories:\n",
            "[\n",
            "    {\n",
            "        \"task\": \"Meanwhile, we must finalize the budget by Friday.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": \"Friday\",\n",
            "        \"cluster\": 0,\n",
            "        \"category\": \"update team new\"\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"Rahul and Priya need to schedule the client call by Monday.\",\n",
            "        \"performer\": \"Priya\",\n",
            "        \"deadline\": \"Monday\",\n",
            "        \"cluster\": 0,\n",
            "        \"category\": \"update team new\"\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"Remember to check the new version of the software for bugs.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": null,\n",
            "        \"cluster\": 0,\n",
            "        \"category\": \"update team new\"\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"The CEO insisted that marketing should distribute the new brochures next week.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": \"next week\",\n",
            "        \"cluster\": 0,\n",
            "        \"category\": \"update team new\"\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"Bob must complete the security review by 5 PM today.\",\n",
            "        \"performer\": \"Bob\",\n",
            "        \"deadline\": \"5 PM today\",\n",
            "        \"cluster\": 0,\n",
            "        \"category\": \"update team new\"\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"Also, Michael should deliver the updated contract by email.\",\n",
            "        \"performer\": \"Michael\",\n",
            "        \"deadline\": null,\n",
            "        \"cluster\": 0,\n",
            "        \"category\": \"update team new\"\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"The CFO said that the financial forecast has to be updated by the end of this quarter.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": \"the end of this quarter\",\n",
            "        \"cluster\": 0,\n",
            "        \"category\": \"update team new\"\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"Could you please forward that email to HR?\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": null,\n",
            "        \"cluster\": 1,\n",
            "        \"category\": \"need track deliverable\"\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"Move the old chairs to the storage room.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": null,\n",
            "        \"cluster\": 0,\n",
            "        \"category\": \"update team new\"\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"We need to keep track of all the software licenses before they expire.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": null,\n",
            "        \"cluster\": 1,\n",
            "        \"category\": \"need track deliverable\"\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"Make sure to call the electrician to fix the broken light.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": null,\n",
            "        \"cluster\": 1,\n",
            "        \"category\": \"need track deliverable\"\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"Maria must write the blog post by Tuesday next week.\",\n",
            "        \"performer\": \"Maria\",\n",
            "        \"deadline\": \"Tuesday next week\",\n",
            "        \"cluster\": 0,\n",
            "        \"category\": \"update team new\"\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"Also, the design team should finalize the logo.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": null,\n",
            "        \"cluster\": 0,\n",
            "        \"category\": \"update team new\"\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"Prepare a backup copy of all website assets.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": null,\n",
            "        \"cluster\": 0,\n",
            "        \"category\": \"update team new\"\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"The cleaning crew must reorder supplies.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": null,\n",
            "        \"cluster\": 0,\n",
            "        \"category\": \"update team new\"\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"The CEO has to review the partnership agreement.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": null,\n",
            "        \"cluster\": 0,\n",
            "        \"category\": \"update team new\"\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"The marketing team should post the social media updates every morning.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": \"every morning\",\n",
            "        \"cluster\": 0,\n",
            "        \"category\": \"update team new\"\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"We must still complete the daily report.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": \"daily\",\n",
            "        \"cluster\": 1,\n",
            "        \"category\": \"need track deliverable\"\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"We need to deliver these tasks on time.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": null,\n",
            "        \"cluster\": 1,\n",
            "        \"category\": \"need track deliverable\"\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"Write the release notes for the software patch.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": null,\n",
            "        \"cluster\": 0,\n",
            "        \"category\": \"update team new\"\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"Timmy must update the user guide with the new features by next Wednesday.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": \"next Wednesday\",\n",
            "        \"cluster\": 0,\n",
            "        \"category\": \"update team new\"\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"The finance team has to finalize the Q1 reports by the end of the month.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": \"the end of the month\",\n",
            "        \"cluster\": 0,\n",
            "        \"category\": \"update team new\"\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"We should not forget to track all deliverables in the project management tool.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": null,\n",
            "        \"cluster\": 1,\n",
            "        \"category\": \"need track deliverable\"\n",
            "    },\n",
            "    {\n",
            "        \"task\": \"We need to send them our feedback.\",\n",
            "        \"performer\": null,\n",
            "        \"deadline\": null,\n",
            "        \"cluster\": 1,\n",
            "        \"category\": \"need track deliverable\"\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample input.txt used\n",
        "___\n",
        "\n",
        "Team 3 will handle the documentation for the new project next week. John is traveling to New York tomorrow.\n",
        "Meanwhile, we must finalize the budget by Friday. Lucy said she would handle the monthly summary.\n",
        "Rahul and Priya need to schedule the client call by Monday. Everyone is excited for the upcoming holiday.\n",
        "Remember to check the new version of the software for bugs. The CEO insisted that marketing should distribute\n",
        "the new brochures next week. Bob must complete the security review by 5 PM today. This is critical for the next release.\n",
        "\n",
        "I want you to vacuum the office space before the guests arrive. The project manager asked us to prepare the slides\n",
        "for tomorrow's presentation. Also, Michael should deliver the updated contract by email. The developers are working\n",
        "on bug fixes. Let's not forget to restock the pantry tomorrow morning. The CFO said that the financial forecast has\n",
        "to be updated by the end of this quarter. Susan is expecting a call from the vendor at 2 PM. We have to decide on\n",
        "a new coffee machine within the next few days. Harriet wants to plan a team outing next month. Could you please\n",
        "forward that email to HR? The security team has recommended that we patch the servers by the end of the day.\n",
        "Everyone is required to sign the new policy documents. Move the old chairs to the storage room.\n",
        "We need to keep track of all the software licenses before they expire. Make sure to call the electrician\n",
        "to fix the broken light. I want you to archive the 2022 records. Maria must write the blog post by Tuesday next week.\n",
        "\n",
        "Also, the design team should finalize the logo. Prepare a backup copy of all website assets.\n",
        "The cleaning crew must reorder supplies. Travis will handle the catering. If anyone can pick up the interns\n",
        "from the train station, let me know. The CEO has to review the partnership agreement. We expect the new interns\n",
        "to fill out their paperwork immediately. The marketing team should post the social media updates every morning.\n",
        "Meanwhile, Rahul goes for coffee every day. There's a possibility that tomorrow might be a holiday.\n",
        "We must still complete the daily report. The manager wants to measure the team's performance.\n",
        "We need to deliver these tasks on time. Write the release notes for the software patch.\n",
        "Timmy must update the user guide with the new features by next Wednesday. The design team is not working\n",
        "on that today. The finance team has to finalize the Q1 reports by the end of the month. We should not forget\n",
        "to track all deliverables in the project management tool. John is hosting a lunch session. Anna might join him.\n",
        "It's crucial that the legal team reviews the compliance guidelines ASAP. We need to send them our feedback."
      ],
      "metadata": {
        "id": "GsMJ9yXjQsIS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART B: Build a machine learning model to classify customer reviews as positive or negative. (30 marks)"
      ],
      "metadata": {
        "id": "0LJPEYSKR9XB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Setup and Library Imports"
      ],
      "metadata": {
        "id": "DsCWIcOmmN3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re, string, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from nltk.corpus import movie_reviews, stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBk5tQGgmPBZ",
        "outputId": "02dd45c6-eb58-42a4-a191-aa2faaf541dc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Data Collection"
      ],
      "metadata": {
        "id": "X07AulogmRpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We use the NLTK movie_reviews dataset as a proxy for customer reviews.\n",
        "# Dataset link: http://www.nltk.org/book/ch02.html\n",
        "# Each review is labeled as either 'pos' (positive) or 'neg' (negative).\n",
        "\n",
        "# Load the dataset and shuffle it for randomness.\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "random.shuffle(documents)\n",
        "\n",
        "# Combine token lists back into strings and create labels:\n",
        "texts = [' '.join(words) for words, label in documents]\n",
        "labels = [1 if label == 'pos' else 0 for words, label in documents]  # 1: positive, 0: negative"
      ],
      "metadata": {
        "id": "NDOs8Q5gm8IA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Data Preprocessing"
      ],
      "metadata": {
        "id": "mTGehQxMm-Ti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to clean the text:\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean the input text by:\n",
        "      - Converting to lowercase.\n",
        "      - Removing digits.\n",
        "      - Removing punctuation.\n",
        "      - Removing extra whitespace.\n",
        "      - Tokenizing, removing stop words, and lemmatizing tokens.\n",
        "    \"\"\"\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove digits\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Tokenize and remove stop words; then lemmatize each token\n",
        "    tokens = text.split()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply the cleaning function to all texts\n",
        "clean_texts = [clean_text(text) for text in texts]"
      ],
      "metadata": {
        "id": "yDFTQxpknAJb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Train/Test Split"
      ],
      "metadata": {
        "id": "7zmMGWnInEbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training (80%) and testing (20%) sets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(clean_texts, labels, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "wtFud6_bnHF3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Model Selection, Training, and Evaluation"
      ],
      "metadata": {
        "id": "CFHGAX6KnJyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will build three pipelines, one for each model.\n",
        "# Each pipeline uses TF-IDF to convert text to numerical features and then applies a classifier.\n",
        "# We use a small grid search to tune hyperparameters.\n",
        "\n",
        "# Create a list to store evaluation results for each model.\n",
        "results = []\n",
        "\n",
        "###########################################\n",
        "# Model 1: Logistic Regression\n",
        "###########################################\n",
        "pipeline_lr = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(ngram_range=(1,2), min_df=5, max_df=0.9, max_features=5000)),\n",
        "    ('clf', LogisticRegression(max_iter=1000, random_state=42))\n",
        "])\n",
        "# Hyperparameter grid for Logistic Regression (tuning regularization strength C)\n",
        "param_grid_lr = {\n",
        "    'clf__C': [0.1, 1]\n",
        "}\n",
        "grid_lr = GridSearchCV(pipeline_lr, param_grid_lr, cv=3, scoring='accuracy', n_jobs=-1)\n",
        "grid_lr.fit(X_train, y_train)\n",
        "y_pred_lr = grid_lr.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
        "prec_lr = precision_score(y_test, y_pred_lr)\n",
        "rec_lr = recall_score(y_test, y_pred_lr)\n",
        "\n",
        "results.append({\n",
        "    'Model': 'Logistic Regression',\n",
        "    'Accuracy': acc_lr,\n",
        "    'Precision': prec_lr,\n",
        "    'Recall': rec_lr,\n",
        "    'Best Params': grid_lr.best_params_\n",
        "})\n",
        "\n",
        "print(\"Logistic Regression Best Params:\", grid_lr.best_params_)\n",
        "\n",
        "###########################################\n",
        "# Model 2: Multinomial Naive Bayes\n",
        "###########################################\n",
        "pipeline_nb = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(ngram_range=(1,2), min_df=5, max_df=0.9, max_features=5000)),\n",
        "    ('clf', MultinomialNB())\n",
        "])\n",
        "# Hyperparameter grid for Naive Bayes (tuning the smoothing parameter alpha)\n",
        "param_grid_nb = {\n",
        "    'clf__alpha': [0.5, 1.0]\n",
        "}\n",
        "grid_nb = GridSearchCV(pipeline_nb, param_grid_nb, cv=3, scoring='accuracy', n_jobs=-1)\n",
        "grid_nb.fit(X_train, y_train)\n",
        "y_pred_nb = grid_nb.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "acc_nb = accuracy_score(y_test, y_pred_nb)\n",
        "prec_nb = precision_score(y_test, y_pred_nb)\n",
        "rec_nb = recall_score(y_test, y_pred_nb)\n",
        "\n",
        "results.append({\n",
        "    'Model': 'Multinomial NB',\n",
        "    'Accuracy': acc_nb,\n",
        "    'Precision': prec_nb,\n",
        "    'Recall': rec_nb,\n",
        "    'Best Params': grid_nb.best_params_\n",
        "})\n",
        "\n",
        "print(\"Multinomial NB Best Params:\", grid_nb.best_params_)\n",
        "\n",
        "###########################################\n",
        "# Model 3: Support Vector Machine (SVM)\n",
        "###########################################\n",
        "pipeline_svm = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(ngram_range=(1,2), min_df=5, max_df=0.9, max_features=5000)),\n",
        "    ('clf', SVC(probability=True, random_state=42))\n",
        "])\n",
        "# Hyperparameter grid for SVM (tuning regularization parameter C and kernel)\n",
        "param_grid_svm = {\n",
        "    'clf__C': [0.1, 1],\n",
        "    'clf__kernel': ['linear']  # Using only the linear kernel for speed\n",
        "}\n",
        "grid_svm = GridSearchCV(pipeline_svm, param_grid_svm, cv=3, scoring='accuracy', n_jobs=-1)\n",
        "grid_svm.fit(X_train, y_train)\n",
        "y_pred_svm = grid_svm.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "acc_svm = accuracy_score(y_test, y_pred_svm)\n",
        "prec_svm = precision_score(y_test, y_pred_svm)\n",
        "rec_svm = recall_score(y_test, y_pred_svm)\n",
        "\n",
        "results.append({\n",
        "    'Model': 'SVM',\n",
        "    'Accuracy': acc_svm,\n",
        "    'Precision': prec_svm,\n",
        "    'Recall': rec_svm,\n",
        "    'Best Params': grid_svm.best_params_\n",
        "})\n",
        "\n",
        "print(\"SVM Best Params:\", grid_svm.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WDchDI-nNLr",
        "outputId": "a288f5a6-da97-420c-cddc-ac21b5e453e7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Best Params: {'clf__C': 1}\n",
            "Multinomial NB Best Params: {'clf__alpha': 1.0}\n",
            "SVM Best Params: {'clf__C': 1, 'clf__kernel': 'linear'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Results Table and Best Model Selection"
      ],
      "metadata": {
        "id": "g1PAID3jnTjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a results DataFrame and sort by Accuracy.\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df[['Model', 'Accuracy', 'Precision', 'Recall', 'Best Params']]\n",
        "results_df = results_df.sort_values(by='Accuracy', ascending=False)\n",
        "print(\"\\n=== Comparison of Models ===\")\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# Determine the best performing model (by highest Accuracy)\n",
        "best_model_name = results_df.iloc[0]['Model']\n",
        "print(\"\\nBest performing model:\", best_model_name)\n",
        "\n",
        "# Print detailed classification report for the best model\n",
        "print(\"\\n=== Detailed Classification Report ===\")\n",
        "if best_model_name == 'Logistic Regression':\n",
        "    best_preds = y_pred_lr\n",
        "elif best_model_name == 'Multinomial NB':\n",
        "    best_preds = y_pred_nb\n",
        "elif best_model_name == 'SVM':\n",
        "    best_preds = y_pred_svm\n",
        "else:\n",
        "    best_preds = None\n",
        "\n",
        "print(classification_report(y_test, best_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqQv-QNFnVqO",
        "outputId": "b9c32911-9c4c-4acd-950b-d5ca8761aace"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Comparison of Models ===\n",
            "              Model  Accuracy  Precision   Recall                            Best Params\n",
            "                SVM    0.8425   0.821782 0.860104 {'clf__C': 1, 'clf__kernel': 'linear'}\n",
            "Logistic Regression    0.8300   0.817259 0.834197                          {'clf__C': 1}\n",
            "     Multinomial NB    0.8075   0.808511 0.787565                    {'clf__alpha': 1.0}\n",
            "\n",
            "Best performing model: SVM\n",
            "\n",
            "=== Detailed Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.83      0.84       207\n",
            "           1       0.82      0.86      0.84       193\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.84      0.84      0.84       400\n",
            "weighted avg       0.84      0.84      0.84       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion on Model Improvement:\n",
        "___\n",
        "- The TF-IDF representation was chosen because it assigns higher weight to words that are\n",
        "  unique to a document, and lower weight to words that occur frequently across all documents,\n",
        "  thus making the features more discriminative than a simple Bag-of-Words model.\n",
        "- Although all three models are fairly simple, further improvements could be made by:\n",
        "  - Experimenting with more hyperparameter combinations or using more folds in cross-validation.\n",
        "  - Incorporating more advanced text preprocessing techniques such as lemmatization (already applied), bigrams/trigrams, or even domain-specific stop word removal.\n",
        "  - Trying ensemble methods that combine multiple classifiers.\n",
        "  - Collecting and incorporating more data to improve generalization.\n",
        "- For this assignment, the best model is chosen based on Accuracy, but precision and recall are also important depending on the business need (e.g., minimizing false negatives)."
      ],
      "metadata": {
        "id": "QX0viQTnn1mJ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}