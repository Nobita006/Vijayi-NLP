{
    "cells": [
     {
      "cell_type": "markdown",
      "id": "BsdZEIbCzWVw",
      "metadata": {},
      "source": [
       "# NLP Pipeline for Task Extraction and Categorization\n",
       "\n",
       "This notebook demonstrates an NLP pipeline that extracts tasks from unstructured text and categorizes them using clustering and topic modeling. The pipeline includes:\n",
       "\n",
       "1. **Preprocessing:** Clean and segment input text into sentences.\n",
       "2. **Task Extraction:** Identify task sentences using heuristics and extract details such as performer and deadline.\n",
       "3. **Clustering:** Compute sentence embeddings, then determine the optimal number of clusters using either the _Elbow Method_ or the _Silhouette Score_.\n",
       "4. **Categorization:** Use LDA topic modeling on each cluster to derive a category label.\n",
       "\n",
       "Follow the cells below to run the pipeline on your sample text (or upload your file)."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bt0pKKwzIwg",
      "metadata": {},
      "source": [
       "# Install required packages\n",
       "!pip install spacy sklearn gensim matplotlib\n",
       "!python -m spacy download en_core_web_md"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "QfM5reV4zAmE",
      "metadata": {},
      "source": [
       "# Import required libraries and load the spaCy model\n",
       "import json\n",
       "import numpy as np\n",
       "import matplotlib.pyplot as plt\n",
       "from pprint import pprint\n",
       "\n",
       "import spacy\n",
       "from spacy.lang.en.stop_words import STOP_WORDS\n",
       "\n",
       "from sklearn.cluster import KMeans\n",
       "from sklearn.metrics import silhouette_score\n",
       "from gensim import corpora, models\n",
       "\n",
       "# Load spaCy model with medium-sized vectors\n",
       "nlp = spacy.load('en_core_web_md')\n",
       "\n",
       "##############################################\n",
       "# Utility Functions\n",
       "##############################################\n",
       "\n",
       "def load_text_from_file(filepath):\n",
       "    \"\"\"\n",
       "    Read text from the provided file path.\n",
       "    \"\"\"\n",
       "    try:\n",
       "        with open(filepath, \"r\", encoding=\"utf-8\") as file:\n",
       "            text = file.read()\n",
       "        return text.strip()\n",
       "    except Exception as e:\n",
       "        raise IOError(f\"Error reading file '{filepath}': {e}\")\n",
       "\n",
       "def pretty_print_tasks(tasks, title=\"Tasks:\"):\n",
       "    \"\"\"\n",
       "    Pretty-print the list of task dictionaries in JSON format.\n",
       "    \"\"\"\n",
       "    print(f\"\\n{title}\")\n",
       "    print(json.dumps(tasks, indent=4))\n",
       "\n",
       "##############################################\n",
       "# Preprocessing Functions\n",
       "##############################################\n",
       "\n",
       "def clean_text(text):\n",
       "    \"\"\"\n",
       "    Clean the input text by stripping whitespace.\n",
       "    \"\"\"\n",
       "    return text.strip()\n",
       "\n",
       "##############################################\n",
       "# Task Identification and Extraction\n",
       "##############################################\n",
       "\n",
       "def is_task_sentence(sentence):\n",
       "    \"\"\"\n",
       "    Determine if a sentence likely represents a task.\n",
       "    Uses two heuristics:\n",
       "      1. Check if the sentence starts with a base form verb.\n",
       "      2. Check for common task-related keywords (e.g., \"has to\", \"please\", \"don't forget\").\n",
       "    \"\"\"\n",
       "    sent_text = sentence.text.strip()\n",
       "    sent_lower = sent_text.lower()\n",
       "    \n",
       "    # Keywords that may indicate a task\n",
       "    task_keywords = ['has to', 'need to', 'needs to', 'should', 'must', 'please', \"don't forget\"]\n",
       "    \n",
       "    # Heuristic 1: Check if the first token is a base form verb\n",
       "    first_token = sentence[0]\n",
       "    if first_token.pos_ == 'VERB' and first_token.tag_ == 'VB':\n",
       "        return True\n",
       "    \n",
       "    # Heuristic 2: Look for task-related keywords\n",
       "    for keyword in task_keywords:\n",
       "        if keyword in sent_lower:\n",
       "            return True\n",
       "    \n",
       "    return False\n",
       "\n",
       "def extract_deadline(sentence):\n",
       "    \"\"\"\n",
       "    Extract deadline information from a sentence by combining adjacent\n",
       "    entities labeled as TIME or DATE.\n",
       "    \"\"\"\n",
       "    deadline_tokens = []\n",
       "    for ent in sentence.ents:\n",
       "        if ent.label_ in [\"TIME\", \"DATE\"]:\n",
       "            deadline_tokens.append(ent.text)\n",
       "    if deadline_tokens:\n",
       "        return \" \".join(deadline_tokens)\n",
       "    return None\n",
       "\n",
       "def extract_task_details(sentence):\n",
       "    \"\"\"\n",
       "    Extract details from a task sentence:\n",
       "      - The full task text\n",
       "      - The performer (first PERSON entity, if any)\n",
       "      - The deadline (if any)\n",
       "    \"\"\"\n",
       "    task_text = sentence.text.strip()\n",
       "    performer = None\n",
       "    \n",
       "    # Find the first PERSON entity\n",
       "    for ent in sentence.ents:\n",
       "        if ent.label_ == \"PERSON\":\n",
       "            performer = ent.text\n",
       "            break\n",
       "    \n",
       "    deadline = extract_deadline(sentence)\n",
       "    return task_text, performer, deadline\n",
       "\n",
       "def process_text(text):\n",
       "    \"\"\"\n",
       "    Process the input text:\n",
       "      - Clean the text\n",
       "      - Segment it into sentences\n",
       "      - Extract sentences that likely represent tasks\n",
       "    Returns a list of dictionaries containing task details.\n",
       "    \"\"\"\n",
       "    cleaned_text = clean_text(text)\n",
       "    doc = nlp(cleaned_text)\n",
       "    tasks = []\n",
       "    \n",
       "    for sent in doc.sents:\n",
       "        if is_task_sentence(sent):\n",
       "            task_text, performer, deadline = extract_task_details(sent)\n",
       "            tasks.append({\n",
       "                \"task\": task_text,\n",
       "                \"performer\": performer,\n",
       "                \"deadline\": deadline\n",
       "            })\n",
       "    return tasks\n",
       "\n",
       "##############################################\n",
       "# Clustering and Categorization Functions\n",
       "##############################################\n",
       "\n",
       "def cluster_tasks(tasks, num_clusters):\n",
       "    \"\"\"\n",
       "    Cluster tasks based on their sentence embeddings using KMeans.\n",
       "    Adds a 'cluster' field to each task.\n",
       "    Returns the updated tasks, the fitted KMeans model, and the task vectors.\n",
       "    \"\"\"\n",
       "    task_vectors = []\n",
       "    for task in tasks:\n",
       "        doc = nlp(task[\"task\"])\n",
       "        task_vectors.append(doc.vector)\n",
       "    X = np.array(task_vectors)\n",
       "    \n",
       "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
       "    kmeans.fit(X)\n",
       "    labels = kmeans.labels_\n",
       "    \n",
       "    for i, task in enumerate(tasks):\n",
       "        task[\"cluster\"] = int(labels[i])\n",
       "    return tasks, kmeans, X\n",
       "\n",
       "def label_clusters_with_lda(tasks, num_topics=1):\n",
       "    \"\"\"\n",
       "    For each cluster, perform LDA topic modeling on the task texts to derive\n",
       "    a category label from the dominant topic. Adds a 'category' field to each task.\n",
       "    \"\"\"\n",
       "    # Group tasks by cluster\n",
       "    cluster_tasks_dict = {}\n",
       "    for task in tasks:\n",
       "        cluster = task[\"cluster\"]\n",
       "        cluster_tasks_dict.setdefault(cluster, []).append(task[\"task\"])\n",
       "    \n",
       "    cluster_labels = {}\n",
       "    for cluster, sentences in cluster_tasks_dict.items():\n",
       "        texts = []\n",
       "        for sentence in sentences:\n",
       "            doc = nlp(sentence)\n",
       "            tokens = [\n",
       "                token.lemma_.lower() \n",
       "                for token in doc \n",
       "                if token.is_alpha and token.text.lower() not in STOP_WORDS\n",
       "            ]\n",
       "            texts.append(tokens)\n",
       "        \n",
       "        dictionary = corpora.Dictionary(texts)\n",
       "        corpus = [dictionary.doc2bow(text) for text in texts]\n",
       "        \n",
       "        if len(dictionary) == 0:\n",
       "            cluster_labels[cluster] = \"General\"\n",
       "            continue\n",
       "        \n",
       "        lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10, random_state=42)\n",
       "        topic_terms = lda_model.show_topic(0, topn=3)\n",
       "        label = \" \".join([word for word, prob in topic_terms])\n",
       "        cluster_labels[cluster] = label\n",
       "    \n",
       "    for task in tasks:\n",
       "        task[\"category\"] = cluster_labels[task[\"cluster\"]]\n",
       "    return tasks, cluster_labels\n",
       "\n",
       "def determine_optimal_clusters_elbow(task_vectors, min_clusters=2, max_clusters=10):\n",
       "    \"\"\"\n",
       "    Use the Elbow Method to display a plot of inertia for different cluster counts.\n",
       "    Adjusts max_clusters if there are fewer samples.\n",
       "    \"\"\"\n",
       "    n_samples = task_vectors.shape[0]\n",
       "    \n",
       "    if n_samples < min_clusters:\n",
       "        print(f\"Warning: Number of samples ({n_samples}) is less than the minimum clusters ({min_clusters}).\")\n",
       "        min_clusters = n_samples\n",
       "    if n_samples < max_clusters:\n",
       "        max_clusters = n_samples\n",
       "\n",
       "    inertias = []\n",
       "    cluster_range = range(min_clusters, max_clusters + 1)\n",
       "    for k in cluster_range:\n",
       "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
       "        kmeans.fit(task_vectors)\n",
       "        inertias.append(kmeans.inertia_)\n",
       "\n",
       "    plt.figure(figsize=(8, 4))\n",
       "    plt.plot(list(cluster_range), inertias, marker=\"o\")\n",
       "    plt.title(\"Elbow Method For Optimal Clusters\")\n",
       "    plt.xlabel(\"Number of clusters\")\n",
       "    plt.ylabel(\"Inertia\")\n",
       "    plt.xticks(list(cluster_range))\n",
       "    plt.grid(True)\n",
       "    plt.show()\n",
       "\n",
       "    print(\"Elbow Method Inertia Values:\")\n",
       "    for k, inertia in zip(cluster_range, inertias):\n",
       "        print(f\"Clusters: {k}, Inertia: {inertia}\")\n",
       "\n",
       "def determine_optimal_clusters_silhouette(task_vectors, min_clusters=2, max_clusters=10):\n",
       "    \"\"\"\n",
       "    Compute the average silhouette score for different numbers of clusters\n",
       "    and return the optimal number of clusters (with the highest score).\n",
       "    Adjusts max_clusters to be at most n_samples - 1.\n",
       "    \"\"\"\n",
       "    n_samples = task_vectors.shape[0]\n",
       "    \n",
       "    if n_samples < min_clusters:\n",
       "        min_clusters = n_samples\n",
       "    # Ensure max_clusters does not exceed n_samples - 1\n",
       "    max_clusters = min(max_clusters, n_samples - 1)\n",
       "\n",
       "    silhouette_scores = []\n",
       "    cluster_range = range(min_clusters, max_clusters + 1)\n",
       "    for k in cluster_range:\n",
       "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
       "        labels = kmeans.fit_predict(task_vectors)\n",
       "        score = silhouette_score(task_vectors, labels)\n",
       "        silhouette_scores.append(score)\n",
       "        print(f\"Clusters: {k}, Silhouette Score: {score:.3f}\")\n",
       "    \n",
       "    optimal_k = cluster_range[np.argmax(silhouette_scores)]\n",
       "    print(f\"\\nOptimal number of clusters based on silhouette score: {optimal_k}\")\n",
       "    return optimal_k\n"
      ]
     },
     {
      "cell_type": "markdown",
      "id": "f4lUY9hczxE_",
      "metadata": {},
      "source": [
       "## Running the Pipeline\n",
       "\n",
       "In the cell below, we define the input text (or you can upload a file) and run the complete pipeline:\n",
       "\n",
       "1. **Task Extraction:** Process the input text to extract tasks.\n",
       "2. **Vectorization:** Compute sentence embeddings for each extracted task.\n",
       "3. **Optimal Cluster Determination:** Use the Silhouette Score (or Elbow Method) to choose the number of clusters.\n",
       "4. **Clustering and Categorization:** Cluster the tasks and assign category labels using LDA.\n",
       "5. **Output:** Display the extracted tasks along with their cluster and category information.\n"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y4Yql55czKGC",
      "metadata": {},
      "source": [
       "# Define the input text (you can also upload a file using Google Colab's file uploader)\n",
       "input_text = \"\"\"\n",
       "Rahul wakes up early every day. He goes to college in the morning and comes back at 3 pm. At present, Rahul is outside. He has to buy the snacks for all of us. Please review the document by tomorrow. John should clean the room by 5 pm today. Don't forget to call Sarah. Mark must submit the report by next Monday.\n",
       "\"\"\"\n",
       "\n",
       "# Uncomment the lines below to upload a file (if preferred) in Google Colab\n",
       "# from google.colab import files\n",
       "# uploaded = files.upload()\n",
       "# for filename in uploaded.keys():\n",
       "#     input_text = load_text_from_file(filename)\n",
       "\n",
       "# Process the input text to extract tasks\n",
       "tasks = process_text(input_text)\n",
       "pretty_print_tasks(tasks, title=\"Extracted Tasks:\")\n",
       "\n",
       "if not tasks:\n",
       "    print(\"No tasks found in the input.\")\n",
       "else:\n",
       "    # Compute task vectors for clustering\n",
       "    task_vectors = []\n",
       "    for task in tasks:\n",
       "        doc = nlp(task[\"task\"])\n",
       "        task_vectors.append(doc.vector)\n",
       "    task_vectors = np.array(task_vectors)\n",
       "\n",
       "    # Choose the method to determine the number of clusters: 'elbow' or 'silhouette'\n",
       "    method = \"silhouette\"  # Change to \"elbow\" if desired\n",
       "\n",
       "    if method == \"elbow\":\n",
       "        print(\"\\nDetermining the optimal number of clusters using the Elbow Method...\")\n",
       "        determine_optimal_clusters_elbow(task_vectors, min_clusters=2, max_clusters=10)\n",
       "        # Manually enter the desired number of clusters after reviewing the plot\n",
       "        num_clusters = int(input(\"Based on the elbow plot, enter the desired number of clusters: \"))\n",
       "    else:\n",
       "        print(\"\\nDetermining the optimal number of clusters using the Silhouette Score...\")\n",
       "        num_clusters = determine_optimal_clusters_silhouette(task_vectors, min_clusters=2, max_clusters=10)\n",
       "\n",
       "    # Cluster the tasks and assign category labels using LDA\n",
       "    tasks, kmeans, _ = cluster_tasks(tasks, num_clusters)\n",
       "    tasks, cluster_labels = label_clusters_with_lda(tasks, num_topics=1)\n",
       "\n",
       "    pretty_print_tasks(tasks, title=\"Tasks with Categories:\")\n"
      ]
     }
    ],
    "metadata": {
     "colab": {
      "name": "Task_Extraction_and_Categorization.ipynb",
      "provenance": []
     },
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "name": "python"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 2
   }
   